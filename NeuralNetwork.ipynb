{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer classification using a neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify many breast cancer diagnosis as benign or malignant according to several features related with the external appearance of the tumours. For that purpose it will be used a machine learning algorithm, in particular a neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import sklearn.linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and create training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load breast cancer database, classified as benign or malignant according to several features\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "#Features (breast_cancer.feature_names): mean radius, mean smoothness, symmetry error...\n",
    "X = breast_cancer.data.T\n",
    "\n",
    "#Labels: 0 is benign and 1 is malignant\n",
    "Y = breast_cancer.target.T\n",
    "Y = np.reshape(Y, (1, Y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 450\n",
      "Number of test examples: 119\n"
     ]
    }
   ],
   "source": [
    "#Training set\n",
    "X_train = X[:, 0:450]\n",
    "Y_train = Y[:, 0:450]\n",
    "\n",
    "#Test set\n",
    "X_test = X[:, 450:]\n",
    "Y_test = Y[:, 450:]\n",
    "\n",
    "#Scale data\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print(\"Number of training examples: \" + str(X_train.shape[1]))\n",
    "print(\"Number of test examples: \" + str(X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network with one hidden layer in order to classifiy cancer diagnostics as benign or malignant. First of all, we define some functions that will be necessary in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X_train.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y_train.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the parameters W and b of our linear model, giving different values for each layer (1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation and cost computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's propagate the calculations in the forward direction, in order to evaluate the cost function before trying to minimize it in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cost = - (1/m) * np.sum(logprobs)\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                   \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In backward propagation we calculate the derivatives, that are needed to use gradient descent algorithm in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gradient descent method, we can update the parameters of our model in order to converge to proper values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    costs = []\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "       \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "                \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0 or 1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693145\n",
      "Cost after iteration 1000: 0.267250\n",
      "Cost after iteration 2000: 0.225739\n",
      "Cost after iteration 3000: 0.209448\n",
      "Cost after iteration 4000: 0.201675\n",
      "Cost after iteration 5000: 0.197127\n",
      "Cost after iteration 6000: 0.193781\n",
      "Cost after iteration 7000: 0.190266\n",
      "Cost after iteration 8000: 0.184760\n",
      "Cost after iteration 9000: 0.177523\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters, costs = nn_model(X_train, Y_train, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot learning curve (with costs)\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X_test)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_test,predictions.T) + np.dot(1-Y_test,1-predictions.T))/float(Y_test.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
